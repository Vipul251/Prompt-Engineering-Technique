Prompt injection and prevention:---Objective:-
How human communicate with each other defined templates basically as instructions as model.
I delve into the intricate world of prompt injection techniques and how to safeguard against them. Prompt Injection Vulnerability is a serious threat where attackers manipulate language models to execute their intentions, potentially leading to data breaches and social engineering exploits.
I discuss two main types of prompt injections: direct and indirect. Direct injections, also known as "jailbreaking," involve attackers tampering with the system prompt itself, potentially accessing backend systems and sensitive data. 

Indirect injections, on the other hand, occur when attackers manipulate external inputs accepted by the language model, hijacking conversations and destabilizing outputs.

Different threats of prompt injection:
Prompt threat from prompt injection
1.Unauthorizzed Information 
2.Fraud Activities:assigned to llm
3.Unauthorized System Access
4.Malware Poisioning
5.Content Manipulation
6.Limited LLM availability


Prompt Inection 
Direct -Jailbreaking(DAN mode ),modeswitching 
Indirect-Token Smuggling ,PI atatcks,code injection,Payload spilting
password llm will ignore so now passcode similar to its like synonyms will  deliberatively just to bypass the detection and mechanism policy and to smugle the llm model to inject the virus
New vulenaribility in large language models.